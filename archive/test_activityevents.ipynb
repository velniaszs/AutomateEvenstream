{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31565412",
   "metadata": {},
   "source": [
    "# Activity Events Fetcher\n",
    "This notebook replicates the functionality of `test-activityevents.ps1` to fetch Power BI activity events based on specific filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fc394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from notebookutils import mssparkutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84115aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Note: The Power BI Activity API typically requires dates to be wrapped in single quotes, e.g., '2024-01-29...'\n",
    "start_date_time = \"'2026-01-01T00:00:00.000Z'\"\n",
    "end_date_time = \"'2026-01-01T12:00:00.000Z'\"\n",
    "\n",
    "# Filters matching the original PowerShell script\n",
    "filters = [\n",
    "    \"Activity eq 'EnableWorkspaceOutboundAccessProtection'\",\n",
    "    \"Activity eq 'DisableWorkspaceOutboundAccessProtection'\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62098a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "# In Azure Fabric, we can often grab the token automatically for the Power BI API\n",
    "auth_token = \"\"\n",
    "\n",
    "if mssparkutils:\n",
    "    try:\n",
    "        # PBI Audience\n",
    "        pbi_audience = \"https://analysis.windows.net/powerbi/api\"\n",
    "        auth_token = mssparkutils.credentials.getToken(pbi_audience)\n",
    "        print(\"Successfully retrieved token from mssparkutils.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get token via mssparkutils: {e}\")\n",
    "\n",
    "if not auth_token:\n",
    "    # Fallback to manual input or validation if automation fails\n",
    "    # You can replace this string with your Bearer token if running manually\n",
    "    auth_token = \"INSERT_YOUR_MANUAL_TOKEN_HERE\" \n",
    "    print(\"Using manual token placeholder. Please replace if mssparkutils failed.\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {auth_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c812945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Data Logic\n",
    "base_url = \"https://api.powerbi.com/v1.0/myorg/admin/activityevents\"\n",
    "all_activity_events = []\n",
    "\n",
    "for f in filters:\n",
    "    if not f or not f.strip():\n",
    "        continue\n",
    "    \n",
    "    # Construct initial URL\n",
    "    # Depending on input, we ensure we construct the query correctly\n",
    "    url = f\"{base_url}?startDateTime={start_date_time}&endDateTime={end_date_time}&$filter={f}\"\n",
    "    \n",
    "    print(f\"Processing filter: {f}\")\n",
    "    \n",
    "    try:\n",
    "        while url:\n",
    "            # print(f\"Fetching: {url}\") # Uncomment for verbose logging\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status() # Raise error for bad status codes\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            # Aggregate results - checking both 'activityEventEntities' and 'value' as per PS script\n",
    "            if 'activityEventEntities' in data:\n",
    "                all_activity_events.extend(data['activityEventEntities'])\n",
    "            elif 'value' in data:\n",
    "                all_activity_events.extend(data['value'])\n",
    "            \n",
    "            # Check for continuation URI for pagination\n",
    "            url = data.get('continuationUri')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing filter '{f}': {e}\")\n",
    "\n",
    "print(f\"Total events fetched: {len(all_activity_events)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and Output\n",
    "if all_activity_events:\n",
    "    try:\n",
    "        # Create Spark DataFrame directly from the list of dictionaries\n",
    "        # PySpark infers schema from the list of row dictionaries\n",
    "        spark_df = spark.createDataFrame(all_activity_events)\n",
    "        \n",
    "        # Display using Fabric's rich display\n",
    "        display(spark_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Spark DataFrame: {e}\")\n",
    "        # Tip: If schema inference fails due to inconsistent types, you might need to define a schema explicity\n",
    "else:\n",
    "    print(\"No events found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ea84d",
   "metadata": {},
   "source": [
    "# Write to Eventhouse (KQL Database)\n",
    "The following cells handle writing the dataframe to an Eventhouse.\n",
    "\n",
    "### Configuration\n",
    "Set your KQL Cluster URI and Database name below.\n",
    "*   **Merge Strategy:** Since KQL is append-optimized, to perform a \"Merge\" (avoid duplicates), we will:\n",
    "    1.  Write the data to a generic **Staging Table**.\n",
    "    2.  Use the `azure-kusto-data` library to execute a KQL command that moves only *new* records (based on `Id`) from Staging to the Final table.\n",
    "    3.  Clear the Staging table.\n",
    "\n",
    "*If you only want to append all data, you can skip the KQL Command part and just write directly to the final table.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d88c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KQL Configuration\n",
    "kusto_cluster_uri = \"https://<your-cluster>.<region>.kusto.data.microsoft.com\"\n",
    "kusto_database = \"<your-database>\"\n",
    "target_table = \"ActivityEvents\"\n",
    "staging_table = \"ActivityEvents_Staging\"\n",
    "\n",
    "try:\n",
    "    # Get Token for Kusto using mssparkutils\n",
    "    kusto_token = mssparkutils.credentials.getToken(kusto_cluster_uri)\n",
    "    print(\"Kusto Token retrieved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get Kusto token: {e}\")\n",
    "    kusto_token = None\n",
    "\n",
    "def run_kusto_command(query):\n",
    "    \"\"\"\n",
    "    Executes a KQL Control Command using the Kusto REST API via the 'requests' library.\n",
    "    This avoids needing the azure-kusto-data library.\n",
    "    \"\"\"\n",
    "    if not kusto_token:\n",
    "        print(\"Cannot run command: No token available.\")\n",
    "        return\n",
    "\n",
    "    # Kusto Management Endpoint\n",
    "    # Ensure URI doesn't have trailing slash\n",
    "    mgmt_endpoint = f\"{kusto_cluster_uri.rstrip('/')}/v1/rest/mgmt\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {kusto_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"db\": kusto_database,\n",
    "        \"csl\": query\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(mgmt_endpoint, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        # print(response.json()) # Uncomment to see detailed result\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute KQL command via REST API: {e}\")\n",
    "        if 'response' in locals() and response.content:\n",
    "             print(f\"Details: {response.content}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_activity_events:\n",
    "    # 1. Write Data to Staging Table using Spark Connector\n",
    "    # Using the Synapse/Fabric Kusto connector with the provided Token\n",
    "    \n",
    "    print(f\"Writing {spark_df.count()} rows to Staging Table: {staging_table}\")\n",
    "    \n",
    "    # Using 'com.microsoft.kusto.spark.synapse.datasource' as recommended for Fabric\n",
    "    spark_df.write \\\n",
    "        .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
    "        .option(\"kustoCluster\", kusto_cluster_uri) \\\n",
    "        .option(\"kustoDatabase\", kusto_database) \\\n",
    "        .option(\"kustoTable\", staging_table) \\\n",
    "        .option(\"accessToken\", kusto_token) \\\n",
    "        .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
    "        .mode(\"Append\") \\\n",
    "        .save()\n",
    "        \n",
    "    print(\"Write to staging complete.\")\n",
    "\n",
    "    # 2. Execute Merge Logic (Deduplication) via REST API helper\n",
    "    \n",
    "    # Construct KQL command to merge\n",
    "    # This query moves rows from Staging to Target WHERE the Id does not already exist in Target\n",
    "    merge_query = f\"\"\"\n",
    "    .set-or-append {target_table} <| \n",
    "    {staging_table} \n",
    "    | join kind=leftanti {target_table} on Id\n",
    "    \"\"\"\n",
    "    \n",
    "    # Command to clean up staging\n",
    "    cleanup_query = f\".clear table {staging_table} data\"\n",
    "\n",
    "    print(\"Executing Merge (set-or-append)...\")\n",
    "    if run_kusto_command(merge_query):\n",
    "        print(\"Merge complete.\")\n",
    "        \n",
    "        print(\"Cleaning up staging table...\")\n",
    "        if run_kusto_command(cleanup_query):\n",
    "            print(\"Cleanup complete.\")\n",
    "    else:\n",
    "        print(\"Merge failed.\")\n",
    "            \n",
    "else:\n",
    "    print(\"No data to write.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
