{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration (Replace with your own values or use mssparkutils.credentials.getSecret)\n",
    "\n",
    "# Environment URL (e.g., https://org1234.crm.dynamics.com)\n",
    "dataverse_env_url = \"https://orgd2bf3532.crm4.dynamics.com\"\n",
    "\n",
    "# Credentials (Use Key Vault in production!)\n",
    "\n",
    "\n",
    "# Construct the Token Scope\n",
    "# Dataverse requires the scope to be the Environment URL + \"/.default\"\n",
    "if not dataverse_env_url.endswith(\"/\"):\n",
    "    token_scope = f\"{dataverse_env_url}/.default\"\n",
    "else:\n",
    "    token_scope = f\"{dataverse_env_url}.default\"\n",
    "\n",
    "authority_url = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "\n",
    "print(f\"Target Environment: {dataverse_env_url}\")\n",
    "print(f\"Auth Scope: {token_scope}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb25f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Authenticate and Get Token (Using MSAL)\n",
    "import msal\n",
    "import requests\n",
    "import json\n",
    "\n",
    "app = msal.ConfidentialClientApplication(\n",
    "    client_id, \n",
    "    authority=authority_url,\n",
    "    client_credential=client_secret\n",
    ")\n",
    "\n",
    "# Acquire token\n",
    "result = app.acquire_token_for_client(scopes=[token_scope])\n",
    "\n",
    "if \"access_token\" in result:\n",
    "    token = result['access_token']\n",
    "    print(\"Authentication successful. Token acquired.\")\n",
    "else:\n",
    "    print(\"Authentication failed.\")\n",
    "    print(result.get(\"error\"))\n",
    "    print(result.get(\"error_description\"))\n",
    "    raise Exception(\"Could not retrieve access token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a190a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Helper Functions: KQL and Dataverse\n",
    "\n",
    "def get_pending_workspace_ids():\n",
    "    \"\"\"\n",
    "    Queries Kusto (ADX) AlertLogs table for distinct WorkspaceIds.\n",
    "    Uses the 'com.microsoft.kusto.spark.synapse.datasource' format.\n",
    "    \"\"\"\n",
    "    # Kusto Configuration\n",
    "    kusto_cluster = \"https://mycluster.kusto.windows.net\"\n",
    "    kusto_database = \"MyDatabase\" # Replace with your actual database name\n",
    "    kusto_query = \"AlertLogs | where AlertStatus != 'EmailSent' | distinct WorkspaceId\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading from Kusto Cluster: {kusto_cluster}, Database: {kusto_database}\")\n",
    "        \n",
    "        # Read from Kusto\n",
    "        df_kql = spark.read.format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
    "            .option(\"kustoCluster\", kusto_cluster) \\\n",
    "            .option(\"kustoDatabase\", kusto_database) \\\n",
    "            .option(\"kustoQuery\", kusto_query) \\\n",
    "            .load()\n",
    "        \n",
    "        # Collect IDs into a python list\n",
    "        workspace_ids = [row.WorkspaceId for row in df_kql.collect()]\n",
    "        \n",
    "        print(f\"Found {len(workspace_ids)} pending workspaces.\")\n",
    "        return workspace_ids\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Kusto: {e}\")\n",
    "        # Return empty list or raising error depending on preference.\n",
    "        return []\n",
    "\n",
    "def fetch_dataverse_account(workspace_id, token, base_url):\n",
    "    \"\"\"\n",
    "    Fetches account details from Dataverse filtering by accountid (mapped to workspaceId).\n",
    "    \"\"\"\n",
    "    api_version = \"v9.2\"\n",
    "    entity_name = \"accounts\"\n",
    "    \n",
    "    # We query for the specific accountid. Using $filter because accountid is the Key.\n",
    "    query_options = f\"?$select=accountid,emailaddress1,emailaddress2&$filter=accountid eq '{workspace_id}'\"\n",
    "    \n",
    "    request_uri = f\"{base_url}/api/data/{api_version}/{entity_name}{query_options}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"OData-MaxVersion\": \"4.0\",\n",
    "        \"OData-Version\": \"4.0\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(request_uri, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if \"value\" in data and len(data[\"value\"]) > 0:\n",
    "            return data[\"value\"][0] # Return the first match\n",
    "        else:\n",
    "            print(f\"WorkspaceId {workspace_id} not found in Dataverse.\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP Error fetching {workspace_id}: {err}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {workspace_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5778212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Main Execution Flow\n",
    "\n",
    "# Step 1: Query AlertLogs for WorkspaceIds\n",
    "print(\"--- Step 1: Querying AlertLogs for Pending Workspaces ---\")\n",
    "pending_workspace_ids = get_pending_workspace_ids()\n",
    "\n",
    "if not pending_workspace_ids:\n",
    "    print(\"No pending workspaces found in AlertLogs (or table not accessible).\")\n",
    "else:\n",
    "    print(f\"Found {len(pending_workspace_ids)} workspaces pending processing.\")\n",
    "\n",
    "    # Step 2: Query Dataverse for each WorkspaceId\n",
    "    print(f\"--- Step 2: Fetching details for {len(pending_workspace_ids)} workspaces from Dataverse ---\")\n",
    "    \n",
    "    account_records = []\n",
    "    base_url = dataverse_env_url.rstrip(\"/\")\n",
    "\n",
    "    for ws_id in pending_workspace_ids:\n",
    "        # Fetch account details\n",
    "        print(f\"Fetching details for WorkspaceId: {ws_id}\")\n",
    "        account_data = fetch_dataverse_account(ws_id, token, base_url)\n",
    "        \n",
    "        if account_data:\n",
    "            # Map Dataverse columns to our Schema\n",
    "            record = {\n",
    "                \"workspaceId\": account_data.get(\"accountid\", ws_id), # Fallback to input ID if missing\n",
    "                \"PrimaryEmail\": account_data.get(\"emailaddress1\"),\n",
    "                \"SecondaryEmail\": account_data.get(\"emailaddress2\")\n",
    "            }\n",
    "            account_records.append(record)\n",
    "\n",
    "    # Step 3: Populate DataFrame and Merge to KQL\n",
    "    if account_records:\n",
    "        print(f\"--- Step 3: Writing {len(account_records)} records to KQL table 'WorkspaceEmail' ---\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df_merged = spark.createDataFrame(account_records)\n",
    "        display(df_merged)\n",
    "\n",
    "        # KQL Configuration\n",
    "        # Ensure these are defined (can be reused from get_pending_workspace_ids or defined globally)\n",
    "        kusto_cluster = \"https://mycluster.kusto.windows.net\" \n",
    "        kusto_database = \"MyDatabase\"\n",
    "        tableName = \"WorkspaceEmail\"\n",
    "\n",
    "        # Write Logic: Using 'set-or-replace' to overwrite specific records isn't directly supported by spark connector in one go like ADX commands.\n",
    "        # But 'append' is the standard mode. To achieve upsert/merge logic in KQL, you typically:\n",
    "        # 1. Append data to a staging table (or the main table).\n",
    "        # 2. Use a materialized view or a stored function with 'arg_max' to get the latest state.\n",
    "        # OR\n",
    "        # Use the '.set-or-append' async command if strictly needed, but from Spark 'append' is standard.\n",
    "        \n",
    "        # Here we will append to the table. KQL is append-only by default.\n",
    "        # De-duplication is handled query-side (using 'summarize arg_max(IngestionTime, *) by WorkspaceId')\n",
    "        \n",
    "        try:\n",
    "            df_merged.write \\\n",
    "                .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
    "                .option(\"kustoCluster\", kusto_cluster) \\\n",
    "                .option(\"kustoDatabase\", kusto_database) \\\n",
    "                .option(\"kustoTable\", tableName) \\\n",
    "                .mode(\"append\") \\\n",
    "                .save()\n",
    "            \n",
    "            print(f\"Successfully appended records to KQL table '{tableName}'.\")\n",
    "            print(\"Note: KQL is append-only. Use 'WorkspaceEmail | summarize arg_max(ingestion_time(), *) by workspaceId' to view latest state.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error writing to KQL: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No valid account records retrieved from Dataverse.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
