{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ac80f8-1b3a-4515-a607-44319f2c9bfe",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-05T13:00:39.7825543Z",
       "execution_start_time": "2026-02-05T13:00:39.4148149Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "5354ddd0-d9aa-46c9-a5b2-d5c8ff1a7825",
       "queued_time": "2026-02-05T13:00:28.3816383Z",
       "session_id": "366a8731-231a-4446-a500-85c908414754",
       "session_start_time": "2026-02-05T13:00:28.3826329Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 366a8731-231a-4446-a500-85c908414754, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from notebookutils import mssparkutils\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dateutil import parser\n",
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aeb14b7-6e7a-4678-a65b-0b2f7287ffad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-05T13:01:13.5608122Z",
       "execution_start_time": "2026-02-05T13:00:39.7849576Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "512746e0-7014-4ad3-bfc4-b0d20eb8f93d",
       "queued_time": "2026-02-05T13:00:28.4565895Z",
       "session_id": "366a8731-231a-4446-a500-85c908414754",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, 366a8731-231a-4446-a500-85c908414754, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing data. Backing up 1h to overlap and catch late arrivals. Resuming from: 2026-02-05 11:32:43.556479+00:00\n",
      "Populating 1 new time windows into OAP_Load_Config...\n",
      "Population complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare Schema and Current Time\n",
    "config_table_name = \"OAP_Load_Config\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"start_date_time\", TimestampType(), True),\n",
    "    StructField(\"end_date_time\", TimestampType(), True),\n",
    "    StructField(\"completion_time\", TimestampType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Use UTC for all time calculations\n",
    "current_time = datetime.now(timezone.utc)\n",
    "\n",
    "# 2. Determine Start Point and Overlap\n",
    "rows_to_insert = []\n",
    "\n",
    "df_config = spark.table(config_table_name)\n",
    "# Check for existing max end_date_time\n",
    "max_row = df_config.agg(max(\"end_date_time\").alias(\"max_end\")).collect()\n",
    "max_end_date = max_row[0][\"max_end\"]\n",
    "\n",
    "if max_end_date:\n",
    "    # If table has data:\n",
    "    # We want to capture late-arriving data by overlapping 1 hour back.\n",
    "    # Instead of a separate \"catch-up\" window, we just back up the start time.\n",
    "    # The main loop logic will ensure windows are split at midnight correctly.\n",
    "    \n",
    "    last_end = max_end_date\n",
    "    if last_end.tzinfo is None:\n",
    "        last_end = last_end.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "    current_start = last_end - timedelta(hours=1)\n",
    "    \n",
    "    print(f\"Found existing data. Backing up 1h to overlap and catch late arrivals. Resuming from: {current_start}\")\n",
    "    \n",
    "else:\n",
    "    # Migrate Old Data (Table is empty)\n",
    "    # Populate 3 days in the past\n",
    "    print(\"No existing data found. Running migration logic for the last 3 days.\")\n",
    "    \n",
    "    # Calculate start date: 3 days ago, aligned to midnight to ensure clean windows.\n",
    "    three_days_ago = current_time - timedelta(days=3)\n",
    "    current_start = three_days_ago.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    if current_start.tzinfo is None:\n",
    "        current_start = current_start.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "    print(f\"Migration start date (aligned to midnight): {current_start}\")\n",
    "\n",
    "# 3. Generate Windows until Current Time\n",
    "# Constraint: A single window cannot cross midnight (Start and End must be on same calendar day).\n",
    "while current_start < current_time:\n",
    "    \n",
    "    # Calculate \"Midnight of the next day\" relative to current_start\n",
    "    # This defines the absolute maximum end time for this specific day.\n",
    "    next_day_date = current_start.date() + timedelta(days=1)\n",
    "    next_midnight = datetime(next_day_date.year, next_day_date.month, next_day_date.day, 0, 0, 0, 0, tzinfo=timezone.utc)\n",
    "    \n",
    "    # Define limit: 1ms before midnight\n",
    "    day_end_limit = next_midnight - timedelta(milliseconds=1)\n",
    "    \n",
    "    # The actual window end is the lesser of the \"End of Day\" or \"Current Time\"\n",
    "    window_end = day_end_limit\n",
    "    \n",
    "    if window_end > current_time:\n",
    "        window_end = current_time\n",
    "        \n",
    "    # Safety break to prevent infinite loops if diff is zero or negative\n",
    "    if window_end <= current_start:\n",
    "        break\n",
    "\n",
    "    # Add row: start, end, completion (None), status ('Pending')\n",
    "    rows_to_insert.append((current_start, window_end, None, \"Pending\"))\n",
    "    \n",
    "    # Advance start to the next millisecond (which is usually Midnight, or just after current_time)\n",
    "    current_start = window_end + timedelta(milliseconds=1)\n",
    "\n",
    "\n",
    "# 4. Write to Lakehouse\n",
    "if rows_to_insert:\n",
    "    print(f\"Populating {len(rows_to_insert)} new time windows into {config_table_name}...\")\n",
    "    new_df = spark.createDataFrame(rows_to_insert, schema=schema)\n",
    "    new_df.write.format(\"delta\").mode(\"append\").saveAsTable(config_table_name)\n",
    "    print(\"Population complete.\")\n",
    "else:\n",
    "    print(\"Configuration table is up to date. No new windows needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b05766-07be-4004-81ef-9551fb6962cd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-05T13:01:13.9093009Z",
       "execution_start_time": "2026-02-05T13:01:13.5630465Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7fd69968-7799-4703-bb26-1f0a463cb918",
       "queued_time": "2026-02-05T13:00:28.5358991Z",
       "session_id": "366a8731-231a-4446-a500-85c908414754",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 366a8731-231a-4446-a500-85c908414754, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved token from mssparkutils.\n"
     ]
    }
   ],
   "source": [
    "# Authentication using Service Principal\n",
    "# Replace these values with your actual Tenant, App ID, and Key Vault details\n",
    "tenant_id = \"your-tenant-id\"\n",
    "client_id = \"your-client-id-app-id\"\n",
    "\n",
    "# RETRIEVE SECRET FROM KEY VAULT (Recommended)\n",
    "# akv_name = \"your-key-vault-name\"\n",
    "# secret_name = \"your-spn-secret-name\"\n",
    "# client_secret = mssparkutils.credentials.getSecret(akv_name, secret_name)\n",
    "\n",
    "# OR HARDCODED (For testing only - typically unsafe)\n",
    "client_secret = \"your-client-secret-value\"\n",
    "\n",
    "def get_spn_token(scope):\n",
    "    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n",
    "    payload = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret,\n",
    "        'scope': scope\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, data=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"access_token\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error retrieving SPN token for scope {scope}: {e}\")\n",
    "        # Print the full error response from Azure AD (contains specific error codes)\n",
    "        print(f\"Response Body: {response.text}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve SPN token for scope {scope}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 1. Get Power BI Token\n",
    "# Note the scope includes /.default for client credentials flow\n",
    "pbi_scope = \"https://analysis.windows.net/powerbi/api/.default\"\n",
    "auth_token = get_spn_token(pbi_scope)\n",
    "\n",
    "if auth_token:\n",
    "    print(\"Successfully retrieved Power BI token via Service Principal.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve Power BI token.\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {auth_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d7fc4-a955-4aea-b3c7-2d74998555b9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-05T13:01:15.3683668Z",
       "execution_start_time": "2026-02-05T13:01:13.9115346Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f3704ebc-045e-4e5c-97fc-301f351b2cb1",
       "queued_time": "2026-02-05T13:00:28.65982Z",
       "session_id": "366a8731-231a-4446-a500-85c908414754",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 366a8731-231a-4446-a500-85c908414754, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kusto Token retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "# KQL Configuration\n",
    "kusto_cluster_uri = \"https://trd-6uegjpfbf030eemxtw.z1.kusto.fabric.microsoft.com\"\n",
    "kusto_database = \"MonitoringEventhouse\"\n",
    "target_table = \"WorkspaceOutboundAccessProtection\"\n",
    "staging_table = \"WorkspaceOutboundAccessProtection_Staging\"\n",
    "\n",
    "try:\n",
    "    # Use the helper function from the previous cell\n",
    "    # Scope for Kusto/ADX is usually {ClusterURI}/.default\n",
    "    kusto_scope = f\"{kusto_cluster_uri}/.default\"\n",
    "    kusto_token = get_spn_token(kusto_scope)\n",
    "    \n",
    "    if kusto_token:\n",
    "        print(\"Kusto Token retrieved via Service Principal.\")\n",
    "    else:\n",
    "        print(\"Failed to get Kusto token (returned None).\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get Kusto token: {e}\")\n",
    "    kusto_token = None\n",
    "\n",
    "def run_kusto_command(query):\n",
    "    \"\"\"\n",
    "    Executes a KQL Control Command using the Kusto REST API via the 'requests' library.\n",
    "    This avoids needing the azure-kusto-data library.\n",
    "    \"\"\"\n",
    "    if not kusto_token:\n",
    "        print(\"Cannot run command: No token available.\")\n",
    "        return\n",
    "\n",
    "    # Kusto Management Endpoint\n",
    "    # Ensure URI doesn't have trailing slash\n",
    "    mgmt_endpoint = f\"{kusto_cluster_uri.rstrip('/')}/v1/rest/mgmt\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {kusto_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"db\": kusto_database,\n",
    "        \"csl\": query\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(mgmt_endpoint, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        # print(response.json()) # Uncomment to see detailed result\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute KQL command via REST API: {e}\")\n",
    "        if 'response' in locals() and response.content:\n",
    "             print(f\"Details: {response.content}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9b6e78-ceab-4f60-b11e-e2029692c8e5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-05T13:01:15.7076444Z",
       "execution_start_time": "2026-02-05T13:01:15.3708212Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "5deddf9d-28ac-4f91-b418-75fb528adc51",
       "queued_time": "2026-02-05T13:00:28.8845198Z",
       "session_id": "366a8731-231a-4446-a500-85c908414754",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, 366a8731-231a-4446-a500-85c908414754, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Processing Function\n",
    "def process_activity_events_window(start_dt_obj, end_dt_obj):\n",
    "    \"\"\"\n",
    "    Fetches data from PBI Activity API for the given window and loads it into KQL.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Format Dates for API (Single Quoted ISO String)\n",
    "    # Ensure they are in UTC\n",
    "    # Manually handle milliseconds to ensure wecapture the exact time (including .999) \n",
    "    # and strictly output 3 decimal places as preferred by many APIs.\n",
    "    # %f produces 6 digits (microseconds), slicing [:-3] gives milliseconds.\n",
    "    s_iso = start_dt_obj.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + \"Z\"\n",
    "    e_iso = end_dt_obj.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + \"Z\"\n",
    "\n",
    "    s_str = f\"'{s_iso}'\"\n",
    "    e_str = f\"'{e_iso}'\"\n",
    "    \n",
    "    print(f\"\\n--- Processing Window: {s_str} to {e_str} ---\")\n",
    "\n",
    "    # 2. Get data from PBI REST API\n",
    "    filters = [\n",
    "        \"Activity eq 'EnableWorkspaceOutboundAccessProtection'\",\n",
    "        \"Activity eq 'DisableWorkspaceOutboundAccessProtection'\"\n",
    "    ]\n",
    "    \n",
    "    base_url = \"https://api.powerbi.com/v1.0/myorg/admin/activityevents\"\n",
    "    all_local_events = []\n",
    "\n",
    "    for f in filters:\n",
    "        if not f or not f.strip():\n",
    "            continue\n",
    "        \n",
    "        url = f\"{base_url}?startDateTime={s_str}&endDateTime={e_str}&$filter={f}\"\n",
    "        \n",
    "        try:\n",
    "            while url:\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                if 'activityEventEntities' in data:\n",
    "                    all_local_events.extend(data['activityEventEntities'])\n",
    "                elif 'value' in data:\n",
    "                    all_local_events.extend(data['value'])\n",
    "                \n",
    "                url = data.get('continuationUri')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing filter '{f}': {e}\")\n",
    "            return False\n",
    "\n",
    "    print(f\"Total events fetched: {len(all_local_events)}\")\n",
    "\n",
    "    if not all_local_events:\n",
    "        #print(\"No events found. Marking as success (empty).\")\n",
    "        return True\n",
    "\n",
    "    # 3. Load data to KQL Table\n",
    "    try:\n",
    "        # Create Spark DataFrame\n",
    "        spark_df_local = spark.createDataFrame(all_local_events)\n",
    "        \n",
    "        #print(f\"Writing {spark_df_local.count()} rows to Staging Table: {staging_table}\")\n",
    "\n",
    "        spark_df_local.write \\\n",
    "            .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
    "            .option(\"kustoCluster\", kusto_cluster_uri) \\\n",
    "            .option(\"kustoDatabase\", kusto_database) \\\n",
    "            .option(\"kustoTable\", staging_table) \\\n",
    "            .option(\"accessToken\", kusto_token) \\\n",
    "            .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
    "            .mode(\"Append\") \\\n",
    "            .save()\n",
    "            \n",
    "        #print(\"Write to staging complete.\")\n",
    "\n",
    "        # Merge Logic\n",
    "        merge_query = f\"\"\"\n",
    "        .set-or-append {target_table} <| \n",
    "        {staging_table} \n",
    "        | join kind=leftanti {target_table} on Id\n",
    "        \"\"\"\n",
    "        \n",
    "        cleanup_query = f\".clear table {staging_table} data\"\n",
    "\n",
    "        # Using the run_kusto_command helper defined in earlier cell\n",
    "        if run_kusto_command(merge_query):\n",
    "            print(\"Merge to target complete.\")\n",
    "            if run_kusto_command(cleanup_query):\n",
    "                print(\"Staging cleanup complete.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Warning: Staging cleanup failed.\")\n",
    "                return False # Or True if you consider data loaded as success regardless of cleanup\n",
    "        else:\n",
    "            print(\"Merge failed.\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error (Load/Merge): {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2d1399-5697-47f0-b028-eb169098d9bb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-05T13:02:18.1385505Z",
       "execution_start_time": "2026-02-05T13:01:15.7098637Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "cd164b34-c763-4543-a490-0eb8cb1138bd",
       "queued_time": "2026-02-05T13:00:29.0089123Z",
       "session_id": "366a8731-231a-4446-a500-85c908414754",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 366a8731-231a-4446-a500-85c908414754, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 pending windows. Starting processing loop...\n",
      "\n",
      "--- Processing Window: '2026-02-05T11:32:43.556Z' to '2026-02-05T13:00:39.944Z' ---\n",
      "Total events fetched: 1\n",
      "Merge to target complete.\n",
      "Staging cleanup complete.\n",
      "Batch processing finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Process Pending Windows\n",
    "# Query pending items from configuration table\n",
    "df_pending = spark.sql(f\"\"\"\n",
    "    SELECT * FROM {config_table_name} \n",
    "    WHERE status != 'Completed' OR status IS NULL \n",
    "    ORDER BY start_date_time ASC\n",
    "\"\"\")\n",
    "\n",
    "pending_list = df_pending.collect()\n",
    "\n",
    "if not pending_list:\n",
    "    print(\"No pending time windows found in configuration table.\")\n",
    "else:\n",
    "    total_items = len(pending_list)\n",
    "    print(f\"Found {total_items} pending windows. Starting processing loop...\")\n",
    "    \n",
    "    for i, row in enumerate(pending_list):\n",
    "        # Extract row values\n",
    "        raw_start = row['start_date_time']\n",
    "        raw_end = row['end_date_time']\n",
    "        \n",
    "        # Run logic\n",
    "        success = process_activity_events_window(raw_start, raw_end)\n",
    "        \n",
    "        if success:\n",
    "            # Update status in Delta Table\n",
    "            # Note: Timestamps in string format usually work with Spark CAST if standard ISO\n",
    "            if raw_start.tzinfo:\n",
    "                # If tz-aware, str() typically includes offset which Spark understands\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                update_cmd = f\"\"\"\n",
    "                    UPDATE {config_table_name}\n",
    "                    SET status = 'Completed', completion_time = current_timestamp()\n",
    "                    WHERE start_date_time = CAST('{raw_start}' AS TIMESTAMP)\n",
    "                      AND end_date_time = CAST('{raw_end}' AS TIMESTAMP)\n",
    "                \"\"\"\n",
    "                spark.sql(update_cmd)\n",
    "                #print(\"Status updated to 'Completed'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to update status in Delta table: {e}\")\n",
    "                # Don't necessarily stop, but it's risky to continue if we can't mark progress\n",
    "                print(\"Stopping loop to prevent reprocessing on next run.\")\n",
    "                break\n",
    "            \n",
    "            # Wait 10s if not the last item\n",
    "            if i < total_items - 1:\n",
    "                print(\"Waiting 10s...\")\n",
    "                time.sleep(10)\n",
    "        else:\n",
    "            print(\"Processing failed for this window. Stopping loop.\")\n",
    "            break\n",
    "            \n",
    "    print(\"Batch processing finished.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "0160fd4d-7e00-4c5e-8995-4a9174b13b63",
    "default_lakehouse_name": "AopConfigLH",
    "default_lakehouse_workspace_id": "611585cb-6332-4849-995e-efce839973f1",
    "known_lakehouses": [
     {
      "id": "0160fd4d-7e00-4c5e-8995-4a9174b13b63"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
